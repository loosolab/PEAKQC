{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Counting Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T14:43:39.898178187Z",
     "start_time": "2023-12-15T14:43:39.854159700Z"
    }
   },
   "source": [
    "bamfile = \"/mnt/workspace2/jdetlef/data/public_data/sorted_heart_left_ventricle_194.bam\"\n",
    "fragments_file = \"/mnt/workspace2/jdetlef/data/public_data/fragments_heart_left_ventricle_194_sorted.bed\"\n",
    "h5ad_file = \"/mnt/workspace2/jdetlef/data/public_data/heart_lv_SM-JF1NY.h5ad\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T14:38:59.522229322Z",
     "start_time": "2023-12-15T14:38:54.585241821Z"
    }
   },
   "source": [
    "import peakqc.general as general\n",
    "import peakqc.insertsizes as insertsizes"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:45:37.197738984Z",
     "start_time": "2023-12-15T15:45:37.148652850Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import datetime\n",
    "from multiprocessing import Manager, Lock, Pool\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "from beartype import beartype\n",
    "import numpy.typing as npt\n",
    "from beartype.typing import Any, Optional, Literal"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "source": [
    "import scanpy as sc"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T14:44:38.389758803Z",
     "start_time": "2023-12-15T14:44:34.966900277Z"
    }
   },
   "source": [
    "adata = sc.read_h5ad(h5ad_file)\n",
    "adata"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T14:46:14.499936670Z",
     "start_time": "2023-12-15T14:46:14.474794834Z"
    }
   },
   "source": [
    "adata_barcodes = adata.obs.index.tolist()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:56:13.154204628Z",
     "start_time": "2023-12-15T15:56:13.136171928Z"
    }
   },
   "source": [
    "%%time\n",
    "# split index for barcodes CBs\n",
    "barcodes = []\n",
    "for entry in adata_barcodes:\n",
    "    barcode = entry.split('+')[1]\n",
    "    barcodes.append(barcode)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "source": [
    "table_from_bam = insertsizes.insertsize_from_bam(bamfile=bamfile,\n",
    "                        barcodes=barcodes,\n",
    "                        barcode_tag='CB',\n",
    "                        chunk_size=100000,\n",
    "                        regions=None)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "source": [
    "table_from_fragments = insertsizes.insertsize_from_fragments(fragments=fragments_file,\n",
    "                              barcodes=barcodes,\n",
    "                              n_threads=8)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "source": [
    "table_from_fragments"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "source": [
    "table_from_fragments.to_hdf('count_table_heart_lv.h5',\n",
    "                            key='df', mode='w')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "source": [
    "table_from_fragments.to_csv('count_table_heart_lv.csv')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "source": [
    "read_table = pd.read_csv('count_table_heart_lv.csv', index_col=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "source": [
    "read_table['dist'][0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "source": [
    "def store_list_to_file(str_list, file_path):\n",
    "    \"\"\"\n",
    "    Stores a list of strings to a file, with each string on a new line.\n",
    "\n",
    "    Args:\n",
    "    str_list (list of str): The list of strings to store.\n",
    "    file_path (str): The path to the file where the list should be stored.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w') as file:\n",
    "        for item in str_list:\n",
    "            file.write(f\"{item}\\n\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "source": [
    "store_list_to_file(barcodes, 'barcodes.txt')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "source": [
    "def read_list_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads a list of strings from a file, assuming each line in the file is a separate string.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): The path to the file to read.\n",
    "\n",
    "    Returns:\n",
    "    list of str: The list of strings read from the file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        return [line.strip() for line in file]\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "source": [
    "len(barcodes)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "source": [
    "read_list_from_file('barcodes.txt')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "source": [
    "barcodes"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "table_from_fragments"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "lock = Lock()\n",
    "type(lock)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "another_lock = Lock()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "type(lock) == type(Lock())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "source": [
    "chunk = insertsize_from_fragments(fragments=fragments_file, barcodes=barcodes,\n",
    "                              n_threads=8)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "source": [
    "# Create a mock lock object\n",
    "lock_instance = Lock()\n",
    "\n",
    "# Call the function with the mock lock\n",
    "insertsizes.init_pool_processes(lock_instance)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "source": [
    "insertsizes._count_fragments_worker(chunk, managed_dict=managed_dict)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "source": [
    "managed_dict['output']['AGGGATAAACCACCGAAGGTCA']['dist'][:10]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "source": [
    "round(managed_dict['output']['AGGGATAAACCACCGAAGGTCA']['mean_insertsize']) == 140"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "source": [
    "managed_dict = {'output': {}}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "source": [
    "chunk.to_csv('example_chunk.csv', index=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "source": [
    "read_chunk = pd.read_csv('example_chunk.csv')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "source": [
    "(chunk == read_chunk).all()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "source": [
    "import os"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "source": [
    "os.getcwd()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "source": [
    "@beartype\n",
    "def _is_gz_file(filepath: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check wheather file is a compressed .gz file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the file is a compressed .gz file.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filepath, 'rb') as test_f:\n",
    "        return test_f.read(2) == b'\\x1f\\x8b'\n",
    "\n",
    "\n",
    "@beartype\n",
    "def init_pool_processes(the_lock: Any) -> None:\n",
    "    '''\n",
    "    Initialize each process with a global variable lock.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    the_lock : Any\n",
    "        Lock object to be used by the processes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    '''\n",
    "    global lock\n",
    "    lock = the_lock\n",
    "\n",
    "\n",
    "@beartype\n",
    "def _check_in_list(element: Any, alist: list[Any] | set[Any]) -> bool:\n",
    "    \"\"\"\n",
    "    Check if element is in list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    element : Any\n",
    "        Element that is checked for.\n",
    "    alist : list[Any] | set[Any]\n",
    "        List or set in which the element is searched for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if element is in list else False\n",
    "    \"\"\"\n",
    "\n",
    "    return element in alist\n",
    "\n",
    "\n",
    "@beartype\n",
    "def _check_true(element: Any, alist: Optional[list[Any]] = None) -> bool:  # true regardless of input\n",
    "    \"\"\"\n",
    "    Return True regardless of input\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    element : Any\n",
    "        Element that is checked for.\n",
    "    alist: Optional[list[Any]]\n",
    "        List or set in which the element is searched for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if element is in list else False\n",
    "    \"\"\"\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "@beartype\n",
    "def _custom_callback(error: Exception) -> None:\n",
    "    \"\"\"\n",
    "    Error callback function for multiprocessing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    error : Exception\n",
    "        Error that is raised.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(error, flush=True)\n",
    "\n",
    "\n",
    "@beartype\n",
    "def insertsize_from_fragments(fragments: str,\n",
    "                              barcodes: Optional[list[str]] = None,\n",
    "                              n_threads: int = 8) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Count the insertsizes of fragments in a fragments file and get basic statistics (mean and total count) per barcode.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fragments : str\n",
    "        Path to fragments file.\n",
    "    barcodes : list[str], optional\n",
    "        List of barcodes to count. If None, all barcodes are counted.\n",
    "    n_threads : int, default 8\n",
    "        Number of threads to use for multiprocessing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Dataframe containing the mean insertsizes and total counts per barcode.\n",
    "    \"\"\"\n",
    "    print('Count insertsizes from fragments...')\n",
    "    # Open fragments file\n",
    "    if _is_gz_file(fragments):\n",
    "        f = gzip.open(fragments, \"rt\")\n",
    "    else:\n",
    "        f = open(fragments, \"r\")\n",
    "\n",
    "    # Prepare function for checking against barcodes list\n",
    "    if barcodes is not None:\n",
    "        barcodes = set(barcodes)\n",
    "        check_in = _check_in_list\n",
    "    else:\n",
    "        check_in = _check_true\n",
    "\n",
    "    # Initialize iterator\n",
    "    iterator = pd.read_csv(fragments,\n",
    "                           delimiter='\\t',\n",
    "                           header=None,\n",
    "                           names=['chr', 'start', 'stop', 'barcode', 'count'],\n",
    "                           iterator=True,\n",
    "                           chunksize=1000000)\n",
    "\n",
    "    # start timer\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # Initialize multiprocessing\n",
    "    m = Manager() # initialize manager\n",
    "    lock = Lock() # initialize lock\n",
    "    managed_dict = m.dict() # initialize managed dict\n",
    "    managed_dict['output'] = {}\n",
    "    # initialize pool\n",
    "    pool = Pool(processes=n_threads,\n",
    "                initializer=init_pool_processes,\n",
    "                initargs=(lock,),\n",
    "                maxtasksperchild=48)\n",
    "    jobs = []\n",
    "    print('Starting counting fragments...')\n",
    "    # split fragments into chunks\n",
    "    for chunk in tqdm(iterator, desc=\"Processing Chunks\"):\n",
    "        return chunk\n",
    "        # apply async job wit callback function\n",
    "        job = pool.apply_async(_count_fragments_worker,\n",
    "                               args=(chunk, barcodes, check_in, managed_dict),\n",
    "                               error_callback=_custom_callback)\n",
    "        jobs.append(job)\n",
    "\n",
    "    # close pool\n",
    "    pool.close()\n",
    "    # wait for all jobs to finish\n",
    "    pool.join()\n",
    "    # reset settings\n",
    "    count_dict = managed_dict['output']\n",
    "\n",
    "    # Close file and print elapsed time\n",
    "    end_time = datetime.datetime.now()\n",
    "    f.close()\n",
    "    elapsed = end_time - start_time\n",
    "    print(\"Done reading file - elapsed time: {0}\".format(str(elapsed).split(\".\")[0]))\n",
    "\n",
    "    # Convert dict to pandas dataframe\n",
    "    print(\"Converting counts to dataframe...\")\n",
    "    table = pd.DataFrame.from_dict(count_dict, orient=\"index\")\n",
    "    # round mean_insertsize to 2 decimals\n",
    "    table[\"mean_insertsize\"] = table[\"mean_insertsize\"].round(2)\n",
    "\n",
    "    print(\"Done getting insertsizes from fragments!\")\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def _count_fragments_worker(chunk: pd.DataFrame,\n",
    "                            barcodes: Optional[list[str]] = None,\n",
    "                            check_in: Any = _check_true,\n",
    "                            managed_dict: dict = {'output': {}}) -> None:\n",
    "    \"\"\"\n",
    "    Worker function for counting fragments.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk : pd.DataFrame\n",
    "        Chunk of fragments file.\n",
    "    barcodes : list[str], optional\n",
    "        List of barcodes to count. If None, all barcodes are counted.\n",
    "    check_in : Any, default _check_true\n",
    "        Function for checking if barcode is in barcodes list.\n",
    "    managed_dict : dict, default None\n",
    "        Dictionary for multiprocessing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize count_dict\n",
    "    count_dict = {}\n",
    "    # Iterate over chunk\n",
    "    for row in chunk.itertuples():\n",
    "        start = int(row[2])\n",
    "        end = int(row[3])\n",
    "        barcode = row[4]\n",
    "        count = int(row[5])\n",
    "        size = end - start - 9  # length of insertion (-9 due to to shifted cutting of Tn5)\n",
    "\n",
    "        # Only add fragment if check is true\n",
    "        if check_in(barcode, barcodes) is True:\n",
    "            count_dict = _add_fragment(count_dict, barcode, size, count) # add fragment to count_dict\n",
    "\n",
    "    # Update managed_dict\n",
    "    lock.acquire() # acquire lock\n",
    "    latest = managed_dict['output']\n",
    "    managed_dict['output'] = _update_count_dict(latest, count_dict) # update managed dict\n",
    "    lock.release() # release lock\n",
    "\n",
    "\n",
    "@beartype\n",
    "def _add_fragment(count_dict: dict[str, int],\n",
    "                  barcode: str,\n",
    "                  size: int,\n",
    "                  count: int = 1,\n",
    "                  max_size: int=1000) -> dict:\n",
    "    \"\"\"\n",
    "    Add fragment of size 'size' to count_dict.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    count_dict : dict[str, int]\n",
    "        Dictionary containing the counts per insertsize.\n",
    "    barcode : str\n",
    "        Barcode of the read.\n",
    "    size : int\n",
    "        Insertsize to add to count_dict.\n",
    "    count : int, default 1\n",
    "        Number of reads to add to count_dict.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Updated count_dict.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize if barcode is seen for the first time\n",
    "    if barcode not in count_dict:\n",
    "        count_dict[barcode] = {\"mean_insertsize\": 0, \"insertsize_count\": 0}\n",
    "\n",
    "    # Add read to dict\n",
    "    if size > 0 and size <= max_size:  # do not save negative insertsize, and set a cap on the maximum insertsize to limit outlier effects\n",
    "\n",
    "        count_dict[barcode][\"insertsize_count\"] += count\n",
    "\n",
    "        # Update mean\n",
    "        mu = count_dict[barcode][\"mean_insertsize\"]\n",
    "        total_count = count_dict[barcode][\"insertsize_count\"]\n",
    "        diff = (size - mu) / total_count\n",
    "        count_dict[barcode][\"mean_insertsize\"] = mu + diff\n",
    "\n",
    "        # Save to distribution\n",
    "        if 'dist' not in count_dict[barcode]:  # initialize distribution\n",
    "            count_dict[barcode]['dist'] = np.zeros(max_size + 1)\n",
    "        count_dict[barcode]['dist'][size] += count # add count to distribution\n",
    "\n",
    "    return count_dict\n",
    "\n",
    "\n",
    "@beartype\n",
    "def _update_count_dict(count_dict_1: dict, count_dict_2: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Updates the managed dict with the new counts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    count_dict_1 : dict\n",
    "        Dictionary containing the counts per insertsize.\n",
    "    count_dict_2 : dict\n",
    "        Dictionary containing the counts per insertsize.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Updated count_dict.\n",
    "    \"\"\"\n",
    "    # Check if count_dict_1 is empty:\n",
    "    if len(count_dict_1) == 0:\n",
    "        return count_dict_2\n",
    "    # Check if count_dict_2 is empty\n",
    "    if len(count_dict_2) == 0:\n",
    "        return count_dict_1\n",
    "\n",
    "    # make Dataframes for computation\n",
    "    df1 = pd.DataFrame(count_dict_1).T\n",
    "    df2 = pd.DataFrame(count_dict_2).T\n",
    "\n",
    "    # merge distributions\n",
    "    combined_dists = df1['dist'].combine(df2['dist'], func=_update_dist)\n",
    "    # merge counts\n",
    "    merged_counts = pd.merge(df1[\"insertsize_count\"], df2[\"insertsize_count\"], left_index=True, right_index=True,\n",
    "                             how='outer').fillna(0)\n",
    "    # sum total counts/barcode\n",
    "    updated_counts = merged_counts.sum(axis=1)\n",
    "\n",
    "    # calculate scaling factors\n",
    "    x_scaling_factor = merged_counts[\"insertsize_count_x\"] / updated_counts\n",
    "    y_scaling_factor = merged_counts[\"insertsize_count_y\"] / updated_counts\n",
    "\n",
    "    # merge mean insertsizes\n",
    "    merged_mean_insertsizes = pd.merge(df1[\"mean_insertsize\"], df2[\"mean_insertsize\"], left_index=True,\n",
    "                                       right_index=True, how='outer').fillna(0)\n",
    "\n",
    "    # scale mean insertsizes\n",
    "    merged_mean_insertsizes[\"mean_insertsize_x\"] = merged_mean_insertsizes[\"mean_insertsize_x\"] * x_scaling_factor\n",
    "    merged_mean_insertsizes[\"mean_insertsize_y\"] = merged_mean_insertsizes[\"mean_insertsize_y\"] * y_scaling_factor\n",
    "\n",
    "    # sum the scaled means\n",
    "    updated_means = merged_mean_insertsizes.sum(axis=1)\n",
    "\n",
    "    # build the updated dictionary\n",
    "    updated_dict = pd.DataFrame(\n",
    "        {'mean_insertsize': updated_means, 'insertsize_count': updated_counts, 'dist': combined_dists}).T.to_dict()\n",
    "\n",
    "    return updated_dict\n",
    "\n",
    "\n",
    "@beartype\n",
    "def _update_dist(dist_1: npt.ArrayLike, dist_2: npt.ArrayLike) -> npt.ArrayLike:\n",
    "    \"\"\"\n",
    "    Updates the Insertsize Distributions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dist_1 : npt.ArrayLike\n",
    "        Insertsize distribution 1.\n",
    "    dist_2 : npt.ArrayLike\n",
    "        Insertsize distribution 2.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    npt.ArrayLike\n",
    "        Updated insertsize distribution.\n",
    "    \"\"\"\n",
    "    # check if both distributions are not empty\n",
    "    if not np.isnan(dist_1).any() and not np.isnan(dist_2).any():\n",
    "        updated_dist = dist_1 + dist_2 # add distributions\n",
    "        return updated_dist.astype(int)\n",
    "    # if one of the distributions is empty, return the other one\n",
    "    elif np.isnan(dist_1).any():\n",
    "        return dist_2.astype(int)\n",
    "    elif np.isnan(dist_2).any():\n",
    "        return dist_1.astype(int)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "insertsize_from_fragments(fragments=fragments_file,\n",
    "                              barcodes=barcodes,\n",
    "                              n_threads = 8)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sctoolbox.tools as tools"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "tools._insertsize_from_fragments(fragments=fragments_file,\n",
    "                              barcodes=barcodes)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import peakqc.general as utils\n",
    "import os\n",
    "import re"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "@beartype\n",
    "def open_bam(file: str,\n",
    "             mode: str,\n",
    "             verbosity: Literal[0, 1, 2, 3] = 3, **kwargs: Any) -> \"pysam.AlignmentFile\":\n",
    "    \"\"\"\n",
    "    Open bam file with pysam.AlignmentFile. On a specific verbosity level.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file : str\n",
    "        Path to bam file.\n",
    "    mode : str\n",
    "        Mode to open the file in. See pysam.AlignmentFile\n",
    "    verbosity : Literal[0, 1, 2, 3], default 3\n",
    "        Set verbosity level. Verbosity level 0 for no messages.\n",
    "    **kwargs : Any\n",
    "        Forwarded to pysam.AlignmentFile\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pysam.AlignmentFile\n",
    "        Object to work on SAM/BAM files.\n",
    "    \"\"\"\n",
    "\n",
    "    # check then load modules\n",
    "    utils.check_module(\"pysam\")\n",
    "    import pysam\n",
    "\n",
    "    # save verbosity, then set temporary one\n",
    "    former_verbosity = pysam.get_verbosity()\n",
    "    pysam.set_verbosity(verbosity)\n",
    "\n",
    "    # open file\n",
    "    handle = pysam.AlignmentFile(file, mode, **kwargs)\n",
    "\n",
    "    # return to former verbosity\n",
    "    pysam.set_verbosity(former_verbosity)\n",
    "\n",
    "    return handle"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "start_run = time.time()\n",
    "\n",
    "regions=None\n",
    "bam = bamfile\n",
    "chunk_size = 100000\n",
    "n_threads=10\n",
    "barcode_tag = 'CB'\n",
    "\n",
    "utils.check_module(\"pysam\")\n",
    "import pysam\n",
    "\n",
    "if isinstance(regions, str):\n",
    "    regions = [regions]\n",
    "\n",
    "# Prepare function for checking against barcodes list\n",
    "if barcodes is not None:\n",
    "    barcodes = set(barcodes)\n",
    "    check_in = _check_in_list\n",
    "else:\n",
    "    check_in = _check_true\n",
    "    \n",
    "# Open bamfile\n",
    "print(\"Opening bam file...\")\n",
    "if not os.path.exists(bam + \".bai\"):\n",
    "    print(\"Bamfile has no index - trying to index with pysam...\")\n",
    "    pysam.index(bam)\n",
    "\n",
    "bam_obj = open_bam(bam, \"rb\", require_index=True)\n",
    "chromosome_lengths = dict(zip(bam_obj.references, bam_obj.lengths))\n",
    "\n",
    "# Create chunked genome regions:\n",
    "print(f\"Creating chunks of size {chunk_size}bp...\")\n",
    "\n",
    "if regions is None:\n",
    "    regions = [f\"{chrom}:0-{length}\" for chrom, length in chromosome_lengths.items()]\n",
    "elif isinstance(regions, str):\n",
    "    regions = [regions]\n",
    "\n",
    "# Create chunks from larger regions\n",
    "regions_split = []\n",
    "for region in regions:\n",
    "    chromosome, start, end = re.split(\"[:-]\", region)\n",
    "    start = int(start)\n",
    "    end = int(end)\n",
    "    for chunk_start in range(start, end, chunk_size):\n",
    "        chunk_end = chunk_start + chunk_size\n",
    "        if chunk_end > end:\n",
    "            chunk_end = end\n",
    "        regions_split.append(f\"{chromosome}:{chunk_start}-{chunk_end}\")\n",
    "        \n",
    "# start timer\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Count insertsize per chunk using multiprocessing\n",
    "print(f\"Counting insertsizes across {len(regions_split)} chunks...\")\n",
    "count_dict = {}\n",
    "read_count = 0\n",
    "#pbar = tqdm(total=len(regions_split), desc=\"Progress: \", unit=\"chunks\")\n",
    "for region in tqdm(regions_split):\n",
    "    chrom, start, end = re.split(\"[:-]\", region)\n",
    "    for read in bam_obj.fetch(chrom, int(start), int(end)):\n",
    "        read_count += 1\n",
    "        try:\n",
    "            barcode = read.get_tag(barcode_tag)\n",
    "        except Exception:  # tag was not found\n",
    "            barcode = \"NA\"\n",
    "\n",
    "        # Add read to dict\n",
    "        if check_in(barcode, barcodes) is True:\n",
    "            size = abs(read.template_length) - 9  # length of insertion\n",
    "            count_dict = _add_fragment(count_dict, barcode, size)     \n",
    "\n",
    "        # Update progress\n",
    "#        pbar.update(1)\n",
    "#    pbar.close()  # close progress bar\n",
    "\n",
    "            \n",
    "# Close file and print elapsed time\n",
    "end_time = datetime.datetime.now()\n",
    "bam_obj.close()\n",
    "elapsed = end_time - start_time\n",
    "print(\"Done reading file - elapsed time: {0}\".format(str(elapsed).split(\".\")[0]))\n",
    "\n",
    "# Convert dict to pandas dataframe\n",
    "print(\"Converting counts to dataframe...\")\n",
    "table = pd.DataFrame.from_dict(count_dict, orient=\"index\")\n",
    "# round mean_insertsize to 2 decimals\n",
    "table[\"mean_insertsize\"] = table[\"mean_insertsize\"].round(2)\n",
    "\n",
    "print(\"Done getting insertsizes from fragments!\")\n",
    "\n",
    "finish_run = time.time()\n",
    "\n",
    "print(f'Run finished in: {finish_run - start_run}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "@beartype\n",
    "def _insertsize_from_bam(bam: str,\n",
    "                         barcode_tag: str = \"CB\",\n",
    "                         barcodes: Optional[list[str]] = None,\n",
    "                         regions: Optional[str | list[str]] = 'chr1:1-2000000',\n",
    "                         chunk_size: int = 100000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get insertsize distributions per barcode from bam file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bam : str\n",
    "        Path to bam file\n",
    "    barcode_tag : str, default \"CB\"\n",
    "        The read tag representing the barcode.\n",
    "    barcodes : Optional[list[str]], default None\n",
    "        List of barcodes to include in the analysis. If None, all barcodes are included.\n",
    "    regions : Optional[str | list[str]], default 'chr1:1-2000000'\n",
    "        Regions to include in the analysis. If None, all reads are included.\n",
    "    chunk_size : int, default 500000\n",
    "        Size of bp chunks to read from bam file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with insertsize distributions per barcode.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError:\n",
    "        1. No reads found in bam-file.\n",
    "        2. If no reads in bam-file overlap with barcodes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load modules\n",
    "    try:\n",
    "        import pysam\n",
    "    except:\n",
    "        print('Check')\n",
    "\n",
    "    if utils._is_notebook() is True:\n",
    "        from tqdm import tqdm_notebook as tqdm\n",
    "    else:\n",
    "        from tqdm import tqdm\n",
    "\n",
    "    if isinstance(regions, str):\n",
    "        regions = [regions]\n",
    "\n",
    "    # Prepare function for checking against barcodes list\n",
    "    if barcodes is not None:\n",
    "        barcodes = set(barcodes)\n",
    "        check_in = _check_in_list\n",
    "    else:\n",
    "        check_in = _check_true\n",
    "\n",
    "    # Open bamfile\n",
    "    logger.info(\"Opening bam file...\")\n",
    "    if not os.path.exists(bam + \".bai\"):\n",
    "        logger.warning(\"Bamfile has no index - trying to index with pysam...\")\n",
    "        pysam.index(bam)\n",
    "\n",
    "    bam_obj = sctoolbox.tools.bam.open_bam(bam, \"rb\", require_index=True)\n",
    "    chromosome_lengths = dict(zip(bam_obj.references, bam_obj.lengths))\n",
    "\n",
    "    # Create chunked genome regions:\n",
    "    logger.info(f\"Creating chunks of size {chunk_size}bp...\")\n",
    "\n",
    "    if regions is None:\n",
    "        regions = [f\"{chrom}:0-{length}\" for chrom, length in chromosome_lengths.items()]\n",
    "    elif isinstance(regions, str):\n",
    "        regions = [regions]\n",
    "\n",
    "    # Create chunks from larger regions\n",
    "    regions_split = []\n",
    "    for region in regions:\n",
    "        chromosome, start, end = re.split(\"[:-]\", region)\n",
    "        start = int(start)\n",
    "        end = int(end)\n",
    "        for chunk_start in range(start, end, chunk_size):\n",
    "            chunk_end = chunk_start + chunk_size\n",
    "            if chunk_end > end:\n",
    "                chunk_end = end\n",
    "            regions_split.append(f\"{chromosome}:{chunk_start}-{chunk_end}\")\n",
    "\n",
    "    # Count insertsize per chunk using multiprocessing\n",
    "    logger.info(f\"Counting insertsizes across {len(regions_split)} chunks...\")\n",
    "    count_dict = {}\n",
    "    read_count = 0\n",
    "    pbar = tqdm(total=len(regions_split), desc=\"Progress: \", unit=\"chunks\")\n",
    "    for region in regions_split:\n",
    "        chrom, start, end = re.split(\"[:-]\", region)\n",
    "        for read in bam_obj.fetch(chrom, int(start), int(end)):\n",
    "            read_count += 1\n",
    "            try:\n",
    "                barcode = read.get_tag(barcode_tag)\n",
    "            except Exception:  # tag was not found\n",
    "                barcode = \"NA\"\n",
    "\n",
    "            # Add read to dict\n",
    "            if check_in(barcode, barcodes) is True:\n",
    "                size = abs(read.template_length) - 9  # length of insertion\n",
    "                count_dict = _add_fragment(count_dict, barcode, size)\n",
    "\n",
    "        # Update progress\n",
    "        pbar.update(1)\n",
    "    pbar.close()  # close progress bar\n",
    "\n",
    "    bam_obj.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import peakqc.general as utils\n",
    "import os\n",
    "import re"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "@beartype\n",
    "def open_bam(file: str,\n",
    "             mode: str,\n",
    "             verbosity: Literal[0, 1, 2, 3] = 3, **kwargs: Any) -> \"pysam.AlignmentFile\":\n",
    "    \"\"\"\n",
    "    Open bam file with pysam.AlignmentFile. On a specific verbosity level.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file : str\n",
    "        Path to bam file.\n",
    "    mode : str\n",
    "        Mode to open the file in. See pysam.AlignmentFile\n",
    "    verbosity : Literal[0, 1, 2, 3], default 3\n",
    "        Set verbosity level. Verbosity level 0 for no messages.\n",
    "    **kwargs : Any\n",
    "        Forwarded to pysam.AlignmentFile\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pysam.AlignmentFile\n",
    "        Object to work on SAM/BAM files.\n",
    "    \"\"\"\n",
    "\n",
    "    # check then load modules\n",
    "    utils.check_module(\"pysam\")\n",
    "    import pysam\n",
    "\n",
    "    # save verbosity, then set temporary one\n",
    "    former_verbosity = pysam.get_verbosity()\n",
    "    pysam.set_verbosity(verbosity)\n",
    "\n",
    "    # open file\n",
    "    handle = pysam.AlignmentFile(file, mode, **kwargs)\n",
    "\n",
    "    # return to former verbosity\n",
    "    pysam.set_verbosity(former_verbosity)\n",
    "\n",
    "    return handle"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "start_run = time.time()\n",
    "\n",
    "regions=None\n",
    "bam = bamfile\n",
    "chunk_size = 10000000\n",
    "n_threads=10\n",
    "cb_tag = 'CB'\n",
    "\n",
    "utils.check_module(\"pysam\")\n",
    "import pysam\n",
    "\n",
    "if isinstance(regions, str):\n",
    "    regions = [regions]\n",
    "\n",
    "# Prepare function for checking against barcodes list\n",
    "if barcodes is not None:\n",
    "    barcodes = set(barcodes)\n",
    "    check_in = _check_in_list\n",
    "else:\n",
    "    check_in = _check_true\n",
    "    \n",
    "# Open bamfile\n",
    "print(\"Opening bam file...\")\n",
    "if not os.path.exists(bam + \".bai\"):\n",
    "    print(\"Bamfile has no index - trying to index with pysam...\")\n",
    "    pysam.index(bam)\n",
    "\n",
    "bam_obj = open_bam(bam, \"rb\", require_index=True)\n",
    "chromosome_lengths = dict(zip(bam_obj.references, bam_obj.lengths))\n",
    "\n",
    "# Create chunked genome regions:\n",
    "print(f\"Creating chunks of size {chunk_size}bp...\")\n",
    "\n",
    "if regions is None:\n",
    "    regions = [f\"{chrom}:0-{length}\" for chrom, length in chromosome_lengths.items()]\n",
    "elif isinstance(regions, str):\n",
    "    regions = [regions]\n",
    "\n",
    "# Create chunks from larger regions\n",
    "regions_split = []\n",
    "for region in regions:\n",
    "    chromosome, start, end = re.split(\"[:-]\", region)\n",
    "    start = int(start)\n",
    "    end = int(end)\n",
    "    for chunk_start in range(start, end, chunk_size):\n",
    "        chunk_end = chunk_start + chunk_size\n",
    "        if chunk_end > end:\n",
    "            chunk_end = end\n",
    "        regions_split.append(f\"{chromosome}:{chunk_start}-{chunk_end}\")\n",
    "        \n",
    "# start timer\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Initialize multiprocessing\n",
    "m = Manager() # initialize manager\n",
    "lock = Lock() # initialize lock\n",
    "managed_dict = m.dict() # initialize managed dict\n",
    "managed_dict['output'] = {}\n",
    "# initialize pool\n",
    "pool = Pool(processes=n_threads,\n",
    "            initializer=init_pool_processes,\n",
    "            initargs=(lock,),\n",
    "            maxtasksperchild=48)\n",
    "jobs = []\n",
    "print('Starting counting fragments...')\n",
    "     \n",
    "tag_idx = None\n",
    "# Count insertsize per chunk using multiprocessing TODO here goes MP\n",
    "for region in tqdm(regions_split):\n",
    "    chrom, start, end = re.split(\"[:-]\", region)\n",
    "    #start_time = time.time()\n",
    "    chunk = list(bam_obj.fetch(chrom, int(start), int(end)))\n",
    "    chunk = [prep_reads(read) for read in chunk]\n",
    "    stop_time = time.time()\n",
    "    #print(f'reading: {stop_time - start_time}')\n",
    "    \n",
    "    job = pool.apply_async(_count_fragments_from_bam_worker,\n",
    "                       args=(chunk, barcodes, check_in, managed_dict),\n",
    "                       error_callback=_custom_callback)\n",
    "    jobs.append(job)\n",
    "\n",
    "# close pool\n",
    "pool.close()\n",
    "# wait for all jobs to finish\n",
    "pool.join()\n",
    "# reset settings\n",
    "count_dict = managed_dict['output']\n",
    "\n",
    "# Close file and print elapsed time\n",
    "end_time = datetime.datetime.now()\n",
    "bam_obj.close()\n",
    "elapsed = end_time - start_time\n",
    "print(\"Done reading file - elapsed time: {0}\".format(str(elapsed).split(\".\")[0]))\n",
    "\n",
    "# Convert dict to pandas dataframe\n",
    "print(\"Converting counts to dataframe...\")\n",
    "table = pd.DataFrame.from_dict(count_dict, orient=\"index\")\n",
    "# round mean_insertsize to 2 decimals\n",
    "table[\"mean_insertsize\"] = table[\"mean_insertsize\"].round(2)\n",
    "\n",
    "print(\"Done getting insertsizes from fragments!\")\n",
    "\n",
    "finish_run = time.time()\n",
    "\n",
    "print(f'Run finished in: {finish_run - start_run}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import time\n",
    "start = time.time()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def _count_fragments_from_bam_worker(chunk: list,\n",
    "                            barcodes: Optional[list[str]] = None,\n",
    "                            check_in: Any = _check_true,\n",
    "                            managed_dict: dict = {'output': {}}) -> None:\n",
    "    \"\"\"\n",
    "    Worker function for counting fragments.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk : pd.DataFrame\n",
    "        Chunk of fragments file.\n",
    "    barcodes : list[str], optional\n",
    "        List of barcodes to count. If None, all barcodes are counted.\n",
    "    check_in : Any, default _check_true\n",
    "        Function for checking if barcode is in barcodes list.\n",
    "    managed_dict : dict, default None\n",
    "        Dictionary for multiprocessing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialize count_dict\n",
    "    count_dict = {}\n",
    "    \n",
    "    # define helper\n",
    "    for [barcode, size] in chunk:\n",
    "    \n",
    "        #barcode = pair[0]\n",
    "        #size = pair[1]\n",
    "        \n",
    "        if check_in(barcode, barcodes) is True:\n",
    "                count_dict = _add_fragment(count_dict, barcode, size) # add fragment to count_dict\n",
    "    \n",
    "    # process\n",
    "    #[process_reads(pair, count_dict) for pair in chunk]\n",
    "    # Update managed_dict\n",
    "    lock.acquire() # acquire lock\n",
    "    latest = managed_dict['output']\n",
    "    try:\n",
    "        managed_dict['output'] = _update_count_dict(latest, count_dict) # update managed dict\n",
    "    except Exception as e:\n",
    "        \n",
    "        print(f'Exception: {e}')\n",
    "    lock.release() # release lock"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def process_reads(pair):\n",
    "    \n",
    "    barcode = pair[0]\n",
    "    size = pair[1]\n",
    "    \n",
    "    if check_in(barcode, barcodes) is True:\n",
    "            count_dict = _add_fragment(count_dict, barcode, size) # add fragment to count_dict\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def prep_reads(read):\n",
    "\n",
    "    barcode = read.get_tag(cb_tag)\n",
    "    size = read.template_length - 9\n",
    "    \n",
    "    return [barcode, size]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sctoolbox.tools as sctools"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "start = time.time()\n",
    "count_table = sctools._insertsize_from_bam(bam=bamfile,\n",
    "                         barcode_tag=\"CB\",\n",
    "                         barcodes=list(barcodes),\n",
    "                         regions=None,\n",
    "                         chunk_size= 100000)\n",
    "\n",
    "stop = time.time()\n",
    "\n",
    "print(f'original implementation: {stop-start}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "count_table.loc['AAATCCGCATAAATGCTACGGG'][np.arange(0,50)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "table.loc['AAATCCGCATAAATGCTACGGG']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "table"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# individual imports\n",
    "import episcanpy as epi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import datetime\n",
    "from multiprocessing import Manager, Lock, Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "from beartype import beartype\n",
    "from beartype.typing import Any, Optional\n",
    "\n",
    "@beartype\n",
    "def _is_gz_file(filepath: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check wheather file is a compressed .gz file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the file is a compressed .gz file.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filepath, 'rb') as test_f:\n",
    "        return test_f.read(2) == b'\\x1f\\x8b'\n",
    "\n",
    "class MPFragmentCounter():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Init class variables.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def init_pool_processes(self, the_lock):\n",
    "        '''\n",
    "        Initialize each process with a global variable lock.\n",
    "        '''\n",
    "        global lock\n",
    "        lock = the_lock\n",
    "\n",
    "    def _check_in_list(self, element: Any, alist: list[Any] | set[Any]) -> bool:\n",
    "        \"\"\"\n",
    "        Check if element is in list.\n",
    "\n",
    "        TODO Do we need this function?\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        element : Any\n",
    "            Element that is checked for.\n",
    "        alist : list[Any] | set[Any]\n",
    "            List or set in which the element is searched for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if element is in list else False\n",
    "        \"\"\"\n",
    "\n",
    "        return element in alist\n",
    "\n",
    "    def _check_true(element: Any, alist: Optional[list[Any]] = None) -> bool:  # true regardless of input\n",
    "        \"\"\"\n",
    "        Return True regardless of input\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        element : Any\n",
    "            Element that is checked for.\n",
    "        alist: Optional[list[Any]]\n",
    "            List or set in which the element is searched for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if element is in list else False\n",
    "        \"\"\"\n",
    "\n",
    "        return True\n",
    "\n",
    "    def custom_callback(self, error):\n",
    "    \tprint(error, flush=True)\n",
    "        \n",
    "\n",
    "    def insertsize_from_fragments(self, fragments: str,\n",
    "                                  barcodes: Optional[list[str]] = None,\n",
    "                                  n_threads: int = 8) -> pd.DataFrame:\n",
    "\n",
    "        print('Count insertsizes from fragments...')\n",
    "        # Open fragments file\n",
    "        if _is_gz_file(fragments):\n",
    "            f = gzip.open(fragments, \"rt\")\n",
    "        else:\n",
    "            f = open(fragments, \"r\")\n",
    "\n",
    "        # Prepare function for checking against barcodes list\n",
    "        if barcodes is not None:\n",
    "            barcodes = set(barcodes)\n",
    "            check_in = self._check_in_list\n",
    "        else:\n",
    "            check_in = self._check_true\n",
    "\n",
    "        iterator = pd.read_csv(fragments,\n",
    "                               delimiter='\\t',\n",
    "                               header=None,\n",
    "                               names=['chr', 'start', 'stop', 'barcode', 'count'],\n",
    "                               iterator=True,\n",
    "                               chunksize=5000000)\n",
    "\n",
    "        # start timer\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Initialize multiprocessing\n",
    "        m = Manager()\n",
    "        lock = Lock()\n",
    "        managed_dict = m.dict()\n",
    "        managed_dict['output'] = {}\n",
    "        pool = Pool(processes=n_threads, initializer=self.init_pool_processes, initargs=(lock,), maxtasksperchild=48)\n",
    "        jobs = []\n",
    "        print('Starting counting fragments...')\n",
    "        # split fragments into chunks\n",
    "        for chunk in tqdm(iterator, desc=\"Processing Chunks\"):\n",
    "            # apply async job wit callback function\n",
    "            job = pool.apply_async(self._count_fragments_worker, args=(chunk, barcodes, check_in, managed_dict), error_callback=self.custom_callback)\n",
    "            jobs.append(job)\n",
    "        \n",
    "        # close pool\n",
    "        pool.close()\n",
    "        # wait for all jobs to finish\n",
    "        pool.join()\n",
    "        # reset settings\n",
    "        count_dict = managed_dict['output']\n",
    "\n",
    "        # Close file and print elapsed time\n",
    "        end_time = datetime.datetime.now()\n",
    "        f.close()\n",
    "\n",
    "        elapsed = end_time - start_time\n",
    "        print(\"Done reading file - elapsed time: {0}\".format(str(elapsed).split(\".\")[0]))\n",
    "\n",
    "        # Convert dict to pandas dataframe\n",
    "        print(\"Converting counts to dataframe...\")\n",
    "        table = pd.DataFrame.from_dict(count_dict, orient=\"index\")\n",
    "        #table = table[[\"insertsize_count\", \"mean_insertsize\"] + sorted(table.columns[2:])]\n",
    "        table[\"mean_insertsize\"] = table[\"mean_insertsize\"].round(2)\n",
    "\n",
    "        print(\"Done getting insertsizes from fragments!\")\n",
    "\n",
    "        return table\n",
    "\n",
    "    def _count_fragments_worker(self, chunk, barcodes, check_in, managed_dict):\n",
    "        \"\"\"\n",
    "        Worker function for counting fragments.\n",
    "        Parameters\n",
    "        ----------\n",
    "        chunk\n",
    "        barcodes\n",
    "        check_in\n",
    "        managed_dict\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize count_dict\n",
    "        count_dict = {}\n",
    "        for row in chunk.itertuples():\n",
    "            start = int(row[2])\n",
    "            end = int(row[3])\n",
    "            barcode = row[4]\n",
    "            count = int(row[5])\n",
    "            size = end - start - 9  # length of insertion (-9 due to to shifted cutting of Tn5)\n",
    "\n",
    "            # Only add fragment if check is true\n",
    "            if check_in(barcode, barcodes) is True:\n",
    "                count_dict = self._add_fragment(count_dict, barcode, size, count)\n",
    "\n",
    "        lock.acquire()\n",
    "        latest = managed_dict['output']\n",
    "        managed_dict['output'] = self._update_count_dict(latest, count_dict)\n",
    "        lock.release()\n",
    "\n",
    "    def _add_fragment(self, count_dict: dict[str, int],\n",
    "                      barcode: str,\n",
    "                      size: int,\n",
    "                      count: int = 1,\n",
    "                      max_size=1000):\n",
    "        \"\"\"\n",
    "        Add fragment of size 'size' to count_dict.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        count_dict : dict[str, int]\n",
    "            Dictionary containing the counts per insertsize.\n",
    "        barcode : str\n",
    "            Barcode of the read.\n",
    "        size : int\n",
    "            Insertsize to add to count_dict.\n",
    "        count : int, default 1\n",
    "            Number of reads to add to count_dict.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize if barcode is seen for the first time\n",
    "        if barcode not in count_dict:\n",
    "            count_dict[barcode] = {\"mean_insertsize\": 0, \"insertsize_count\": 0}\n",
    "\n",
    "        # Add read to dict\n",
    "        if size > 0 and size <= max_size:  # do not save negative insertsize, and set a cap on the maximum insertsize to limit outlier effects\n",
    "\n",
    "            count_dict[barcode][\"insertsize_count\"] += count\n",
    "\n",
    "            # Update mean\n",
    "            mu = count_dict[barcode][\"mean_insertsize\"]\n",
    "            total_count = count_dict[barcode][\"insertsize_count\"]\n",
    "            diff = (size - mu) / total_count\n",
    "            count_dict[barcode][\"mean_insertsize\"] = mu + diff\n",
    "\n",
    "            # Save to distribution\n",
    "            if 'dist' not in count_dict[barcode]:  # first time size is seen\n",
    "                count_dict[barcode]['dist'] = np.zeros(max_size+1)\n",
    "            count_dict[barcode]['dist'][size] += count\n",
    "\n",
    "        return count_dict\n",
    "\n",
    "    def _update_count_dict(self, count_dict_1, count_dict_2):\n",
    "        \"\"\"\n",
    "        updates\n",
    "        \"\"\"\n",
    "        # Check if count_dict_1 is empty:\n",
    "        if len(count_dict_1) == 0:\n",
    "            return count_dict_2\n",
    "\n",
    "        # make Dataframes for computation\n",
    "        df1 = pd.DataFrame(count_dict_1).T\n",
    "        df2 = pd.DataFrame(count_dict_2).T\n",
    "\n",
    "        # merge distributions\n",
    "        combined_dists = df1['dist'].combine(df2['dist'], func=self._update_dist)\n",
    "        # merge counts\n",
    "        merged_counts = pd.merge(df1[\"insertsize_count\"], df2[\"insertsize_count\"], left_index=True, right_index=True,\n",
    "                                 how='outer').fillna(0)\n",
    "        # sum total counts/barcode\n",
    "        updated_counts = merged_counts.sum(axis=1)\n",
    "\n",
    "        # calculate scaling factors\n",
    "        x_scaling_factor = merged_counts[\"insertsize_count_x\"] / updated_counts\n",
    "        y_scaling_factor = merged_counts[\"insertsize_count_y\"] / updated_counts\n",
    "\n",
    "        # merge mean insertsizes\n",
    "        merged_mean_insertsizes = pd.merge(df1[\"mean_insertsize\"], df2[\"mean_insertsize\"], left_index=True,\n",
    "                                           right_index=True, how='outer').fillna(0)\n",
    "\n",
    "        # scale mean insertsizes\n",
    "        merged_mean_insertsizes[\"mean_insertsize_x\"] = merged_mean_insertsizes[\"mean_insertsize_x\"] * x_scaling_factor\n",
    "        merged_mean_insertsizes[\"mean_insertsize_y\"] = merged_mean_insertsizes[\"mean_insertsize_y\"] * y_scaling_factor\n",
    "\n",
    "        # sum the scaled means\n",
    "        updated_means = merged_mean_insertsizes.sum(axis=1)\n",
    "\n",
    "        # build the updated dictionary\n",
    "        updated_dict = pd.DataFrame(\n",
    "            {'mean_insertsize': updated_means, 'insertsize_count': updated_counts, 'dist': combined_dists}).T.to_dict()\n",
    "\n",
    "        return updated_dict\n",
    "\n",
    "\n",
    "    def _update_dist(self, dist_1, dist_2):\n",
    "        \"\"\"Updates the Insertsize Distributions\"\"\"\n",
    "        if not np.isnan(dist_1).any() and not np.isnan(dist_2).any():\n",
    "            updated_dist = dist_1 + dist_2\n",
    "            return updated_dist.astype(int)\n",
    "        elif np.isnan(dist_1).any():\n",
    "            return dist_2.astype(int)\n",
    "        elif np.isnan(dist_2).any():\n",
    "            return dist_1.astype(int)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def get_dist_df(dist):\n",
    "    \n",
    "    table_dict = {}\n",
    "    for row in dist.iterrows():\n",
    "        barcode = str(row[0])\n",
    "        table_dict[barcode] = {}\n",
    "\n",
    "        for i, counts in enumerate(row[1]['dist']):\n",
    "            table_dict[barcode][i] = counts\n",
    "    \n",
    "    dist_df = pd.DataFrame(table_dict).T\n",
    "    \n",
    "    return dist_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "adata_barcodes = adata.obs.index.tolist()\n",
    "# split index for barcodes CBs\n",
    "barcodes = []\n",
    "for entry in adata_barcodes:\n",
    "    barcode = entry.split('+')[1]\n",
    "    barcodes.append(barcode)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "counter = MPFragmentCounter()\n",
    "table_mp = counter.insertsize_from_fragments(fragments_file, barcodes, n_threads=10)\n",
    "print(table_mp)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "table_mp.loc['AAATCCGCATAAACGTCCCGTT']['dist'].sum()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "table_sctoolbox = tools._insertsize_from_fragments(fragments_file, barcodes)\n",
    "print(table_sctoolbox)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "table_sctoolbox.loc['AAATCCGCATAAACGTCCCGTT']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "table_sctoolbox.loc['AAATCCGCATAAACGTCCCGTT'][[c for c in table_sctoolbox.columns if isinstance(c, int)]].sum()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "table_sctoolbox.loc['AAATCCGCATAAACGTCCCGTT'][[c for c in table_sctoolbox.columns if isinstance(c, int)]][0:50]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "table_sctoolbox = table_sctoolbox[[c for c in table_sctoolbox.columns if isinstance(c, int)]]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "table_mp = get_dist_df(table_mp)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "table_sctoolbox.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "table_mp.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "sorted_table_mp = table_mp.sort_index()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "sorted_table_mp"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "sorted_table_sctoolbox = table_sctoolbox.sort_index()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "sorted_table_sctoolbox"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "sorted_table_mp.equals(sorted_table_sctoolbox)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "sorted_table_mp == table_mp"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:57:38.120801356Z",
     "start_time": "2023-12-15T15:57:38.072063844Z"
    }
   },
   "source": [
    "def count_lines(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return sum(1 for line in file)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:57:54.544122099Z",
     "start_time": "2023-12-15T15:57:39.446642696Z"
    }
   },
   "source": [
    "%%time\n",
    "# Replace 'yourfile.txt' with the path to your file\n",
    "number_of_lines = count_lines(fragments_file)\n",
    "print(f\"Total number of lines: {number_of_lines}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:45:22.195137291Z",
     "start_time": "2023-12-15T15:45:22.182267393Z"
    }
   },
   "source": [
    "#small_fragments = '/mnt/workspace2/jdetlef/data/public_data/cropped_heart_fragments.bed'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "small_fragments = '/home/jan/Workspace/bio_data/small_fragments.bed'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:51:15.418105880Z",
     "start_time": "2023-12-15T15:51:15.388304546Z"
    }
   },
   "source": [
    "\n",
    "class MPFragmentCounter():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Init class variables.\"\"\"\n",
    "        \n",
    "        self.m = Manager()\n",
    "        self.d = self.m.dict()\n",
    "        self.d['output'] = {}\n",
    "        self.lock = Lock()\n",
    "\n",
    "\n",
    "        \n",
    "    def _check_in_list(element: Any, alist: list[Any] | set[Any]) -> bool:\n",
    "        \"\"\"\n",
    "        Check if element is in list.\n",
    "\n",
    "        TODO Do we need this function?\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        element : Any\n",
    "            Element that is checked for.\n",
    "        alist : list[Any] | set[Any]\n",
    "            List or set in which the element is searched for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if element is in list else False\n",
    "        \"\"\"\n",
    "\n",
    "        return element in alist\n",
    "\n",
    "\n",
    "    \n",
    "    def _check_true(element: Any, alist: Optional[list[Any]] = None) -> bool:  # true regardless of input\n",
    "        \"\"\"\n",
    "        Return True regardless of input\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        element : Any\n",
    "            Element that is checked for.\n",
    "        alist: Optional[list[Any]]\n",
    "            List or set in which the element is searched for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if element is in list else False\n",
    "        \"\"\"\n",
    "\n",
    "        return True\n",
    "\n",
    "    \n",
    "    def insertsize_from_fragments(self, fragments: str,\n",
    "                                  barcodes: Optional[list[str]] = None,\n",
    "                                  n_threads: int = 8) -> pd.DataFrame:\n",
    "        # Open fragments file\n",
    "        if _is_gz_file(fragments):\n",
    "            f = gzip.open(fragments, \"rt\")\n",
    "        else:\n",
    "            f = open(fragments, \"r\")\n",
    "\n",
    "        # Prepare function for checking against barcodes list\n",
    "        if barcodes is not None:\n",
    "            barcodes = set(barcodes)\n",
    "            check_in = self._check_in_list\n",
    "        else:\n",
    "            check_in = self._check_true\n",
    "\n",
    "        iterator = pd.read_csv(fragments,\n",
    "                               delimiter='\\t',\n",
    "                               header=None,\n",
    "                               names=['chr', 'start', 'stop', 'barcode', 'count'],\n",
    "                               iterator=True,\n",
    "                               chunksize=1000)\n",
    "\n",
    "        # start timer\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        pool = Pool(n_threads, maxtasksperchild=48)\n",
    "        jobs = []\n",
    "        # split fragments into chunks\n",
    "        for chunk in iterator:\n",
    "            # apply async job wit callback function\n",
    "            job = pool.apply_async(self._count_fragments_worker, args=(chunk, barcodes, check_in))\n",
    "            jobs.append(job)\n",
    "        # monitor progress\n",
    "        # utils.monitor_jobs(jobs, description=\"Progress\")\n",
    "        # close pool\n",
    "        pool.close()\n",
    "        # wait for all jobs to finish\n",
    "        pool.join()\n",
    "        # reset settings\n",
    "        count_dict = self.d\n",
    "        print('what is going on')\n",
    "        print(count_dict)\n",
    "        # Fill missing sizes with 0\n",
    "        max_fragment_size = 1001\n",
    "\n",
    "        for barcode in count_dict:\n",
    "            for size in range(max_fragment_size):\n",
    "                if size not in count_dict[barcode]:\n",
    "                    count_dict[barcode][size] = 0\n",
    "\n",
    "        # Close file and print elapsed time\n",
    "        end_time = datetime.datetime.now()\n",
    "        f.close()\n",
    "\n",
    "        elapsed = end_time - start_time\n",
    "        print(\"Done reading file - elapsed time: {0}\".format(str(elapsed).split(\".\")[0]))\n",
    "\n",
    "        # Convert dict to pandas dataframe\n",
    "        print(\"Converting counts to dataframe...\")\n",
    "        table = pd.DataFrame.from_dict(count_dict, orient=\"index\")\n",
    "        table = table[[\"insertsize_count\", \"mean_insertsize\"] + sorted(table.columns[2:])]\n",
    "        table[\"mean_insertsize\"] = table[\"mean_insertsize\"].round(2)\n",
    "\n",
    "        print(\"Done getting insertsizes from fragments!\")\n",
    "\n",
    "        return table\n",
    "\n",
    "    \n",
    "    def _count_fragments_worker(self, chunk, barcodes, check_in):\n",
    "        \n",
    "        count_dict = {}\n",
    "        \n",
    "        for i in range(len(chunk)):\n",
    "            row = chunk.iloc[i]\n",
    "            start = int(row['start'])\n",
    "            end = int(row['stop'])\n",
    "            barcode = row['barcode']\n",
    "            count = int(row['count'])\n",
    "            size = end - start - 9  # length of insertion (-9 due to to shifted cutting of Tn5)\n",
    "\n",
    "            # Only add fragment if check is true\n",
    "            if check_in(barcode, barcodes) is True:\n",
    "                count_dict = self._add_fragment(count_dict, barcode, size, count)\n",
    "                \n",
    "        with self.lock:\n",
    "            self.d['output'] = update_count_dict(self.d['output'], count_dict)\n",
    "\n",
    "\n",
    "    def _add_fragment(count_dict: dict[str, int],\n",
    "                      barcode: str,\n",
    "                      size: int,\n",
    "                      count: int = 1):\n",
    "        \"\"\"\n",
    "        Add fragment of size 'size' to count_dict.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        count_dict : dict[str, int]\n",
    "            Dictionary containing the counts per insertsize.\n",
    "        barcode : str\n",
    "            Barcode of the read.\n",
    "        size : int\n",
    "            Insertsize to add to count_dict.\n",
    "        count : int, default 1\n",
    "            Number of reads to add to count_dict.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize if barcode is seen for the first time\n",
    "        if barcode not in count_dict:\n",
    "            count_dict[barcode] = {\"mean_insertsize\": 0, \"insertsize_count\": 0}\n",
    "\n",
    "        # Add read to dict\n",
    "        if size >= 0 and size <= 1000:  # do not save negative insertsize, and set a cap on the maximum insertsize to limit outlier effects\n",
    "\n",
    "            count_dict[barcode][\"insertsize_count\"] += count\n",
    "\n",
    "            # Update mean\n",
    "            mu = count_dict[barcode][\"mean_insertsize\"]\n",
    "            total_count = count_dict[barcode][\"insertsize_count\"]\n",
    "            diff = (size - mu) / total_count\n",
    "            count_dict[barcode][\"mean_insertsize\"] = mu + diff\n",
    "\n",
    "            # Save to distribution\n",
    "            if size not in count_dict[barcode]:  # first time size is seen\n",
    "                count_dict[barcode][size] = 0\n",
    "            count_dict[barcode][size] += count\n",
    "            \n",
    "        return count_dict\n",
    "    \n",
    "\n",
    "    def _log_result(self, result: Any) -> None:\n",
    "        \"\"\"Log results from mp_counter.\"\"\"\n",
    "\n",
    "        if self.merged_dict:\n",
    "            self.merged_dict = dict(Counter(self.merged_dict) + Counter(result))\n",
    "            # print('merging')\n",
    "        else:\n",
    "            self.merged_dict = result"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    " mpc = MPFragmentCounter()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "counts = mpc.insertsize_from_fragments(small_fragments, barcodes)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "some_dict = {}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "some_dict['another'] = {'test': 'Hallo'}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "some_dict['another']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "count_dict={}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "count_dict_1={}\n",
    "count_dict_1['ACGTT'] = {\"mean_insertsize\": 10, \"insertsize_count\": 5, 'dist': np.array([0,1,0,2,1,1,0])}\n",
    "count_dict_1['GTCCT'] = {\"mean_insertsize\": 10, \"insertsize_count\": 20, 'dist': np.array([0,0,0,1,2,2,1])}\n",
    "count_dict_1['GCGCG'] = {\"mean_insertsize\": 10, \"insertsize_count\": 20, 'dist': np.array([0,0,0,1,2,2,1])}\n",
    "\n",
    "count_dict_2={}\n",
    "count_dict_2['ACGTT'] = {\"mean_insertsize\": 20, \"insertsize_count\": 20, 'dist': np.array([2,1,1,0,1,1,0])}\n",
    "count_dict_2['GTCCT'] = {\"mean_insertsize\": 20, \"insertsize_count\": 5, 'dist': np.array([1,0,2,2,1,1,0])}\n",
    "count_dict_2['TTTAA'] = {\"mean_insertsize\": 20, \"insertsize_count\": 5, 'dist': np.array([1,0,2,2,1,1,0])}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# make Dataframes for computation\n",
    "df1 = pd.DataFrame(count_dict_1).T\n",
    "df2 = pd.DataFrame(count_dict_2).T\n",
    "\n",
    "# merge counts\n",
    "combined_dists = df1['dist'].combine(df2['dist'], func=update_dist)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "merged_counts = pd.merge(df1[\"insertsize_count\"], df2[\"insertsize_count\"], left_index=True, right_index=True, how='outer').fillna(0)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "merged_counts"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "    # merge counts\n",
    "    merged_counts = pd.merge(df1[\"insertsize_count\"], df2[\"insertsize_count\"], left_index=True, right_index=True, how='outer').fillna(0)\n",
    "    # sum total counts/barcode\n",
    "    updated_counts = merged_counts.sum(axis=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "df_dists= pd.DataFrame({'combined_dists' : combined_dists})"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "df_counts = pd.DataFrame({'insertsize_counts' : updated_counts})"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "df_counts = pd.DataFrame({'insertsize_counts': {'TTTAA':20, 'ACGTT':25, 'GCGCG': 25, 'GTCCT': 5}})"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "df_counts"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "some_dict = {}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "len(some_dict)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def update_count_dict(count_dict_1, count_dict_2):\n",
    "    \"\"\"\n",
    "    updates\n",
    "    \"\"\"\n",
    "    # Check if count_dict_1 is empty:\n",
    "    if len(count_dict_1) == 0:\n",
    "        return count_dict_2\n",
    "        \n",
    "    # make Dataframes for computation\n",
    "    df1 = pd.DataFrame(count_dict_1).T\n",
    "    df2 = pd.DataFrame(count_dict_2).T\n",
    "\n",
    "    # merge distributions\n",
    "    combined_dists = df1['dist'].combine(df2['dist'], func=update_dist)\n",
    "    \n",
    "    # merge counts\n",
    "    merged_counts = pd.merge(df1[\"insertsize_count\"], df2[\"insertsize_count\"], left_index=True, right_index=True, how='outer').fillna(0)\n",
    "    # sum total counts/barcode\n",
    "    updated_counts = merged_counts.sum(axis=1)\n",
    "    \n",
    "\n",
    "    # calculate scaling factors\n",
    "    x_scaling_factor = merged_counts[\"insertsize_count_x\"] / updated_counts\n",
    "    y_scaling_factor = merged_counts[\"insertsize_count_y\"] / updated_counts\n",
    "\n",
    "    # merge mean insertsizes\n",
    "    merged_mean_insertsizes = pd.merge(df1[\"mean_insertsize\"], df2[\"mean_insertsize\"], left_index=True, right_index=True, how='outer').fillna(0)\n",
    "\n",
    "    # scale mean insertsizes\n",
    "    merged_mean_insertsizes[\"mean_insertsize_x\"] = merged_mean_insertsizes[\"mean_insertsize_x\"] * x_scaling_factor\n",
    "    merged_mean_insertsizes[\"mean_insertsize_y\"] = merged_mean_insertsizes[\"mean_insertsize_y\"] * y_scaling_factor\n",
    "\n",
    "    # sum the scaled means\n",
    "    updated_means = merged_mean_insertsizes.sum(axis=1)\n",
    "\n",
    "    # build the updated dictionary\n",
    "    updated_dict = pd.DataFrame({'mean_insertsize': updated_means, 'insertsize_count' : updated_counts, 'dist': combined_dists}).T.to_dict()\n",
    "    \n",
    "    \n",
    "    return updated_dict\n",
    "\n",
    "\n",
    "def update_dist(dist_1, dist_2):\n",
    "    \"\"\"Updates the Insertsize Distributions\"\"\"\n",
    "    if not np.isnan(dist_1).any() and not np.isnan(dist_2).any():\n",
    "        updated_dist = dist_1 + dist_2\n",
    "        return updated_dist\n",
    "    elif np.isnan(dist_1).any():\n",
    "        return dist_2\n",
    "    elif np.isnan(dist_2).any():\n",
    "        return dist_1"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "pd.DataFrame({'mean_insertsizes': updated_means, 'insertsize_counts' : updated_counts})"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "np.array([1,3,21,0]) / 10"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "merged_insertsizes = pd.merge(df1[\"insertsize_count\"], df2[\"insertsize_count\"], left_index=True, right_index=True)\n",
    "merged"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "x_scaling_factor = merged_insertsizes[\"insertsize_count_x\"] / merged_insertsizes.sum(axis=1)\n",
    "y_scaling_factor = merged_insertsizes[\"insertsize_count_y\"] / merged_insertsizes.sum(axis=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "merged_mean_insertsizes = pd.merge(df1[\"mean_insertsize\"], df2[\"mean_insertsize\"], left_index=True, right_index=True)\n",
    "merged_mean_insertsizes"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "merged_mean_insertsizes[\"mean_insertsize_x\"] = merged_mean_insertsizes[\"mean_insertsize_x\"] * x_scaling_factor\n",
    "merged_mean_insertsizes[\"mean_insertsize_y\"] = merged_mean_insertsizes[\"mean_insertsize_y\"] * y_scaling_factor"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "merged_mean_insertsizes.sum(axis=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "merged_mean_insertsizes * "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Erstellen Sie zwei Beispieldatenframes\n",
    "df1 = pd.DataFrame({'Werte1': [1, 2, 3]}, index=['a', 'b', 'c'])\n",
    "df2 = pd.DataFrame({'Werte1': [4, 5, 6]}, index=['a', 'b', 'c'])\n",
    "\n",
    "# Mergen Sie die DataFrames am Index\n",
    "merged_df = pd.merge(df1, df2, left_index=True, right_index=True)\n",
    "\n",
    "# Summieren Sie die Werte\n",
    "summed_df = merged_df.sum(axis=1)\n",
    "\n",
    "print(summed_df)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "merged_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T14:54:55.792910295Z",
     "start_time": "2023-12-15T14:49:43.045852210Z"
    }
   },
   "source": [
    "%%time\n",
    "count_table = tools._insertsize_from_fragments(small_fragments, barcodes)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "@beartype\n",
    "def _insertsize_from_fragments(fragments: str,\n",
    "                               barcodes: Optional[list[str]] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get fragment insertsize distributions per barcode from fragments file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fragments : str\n",
    "        Path to fragments.bed(.gz) file.\n",
    "    barcodes : Optional[list[str]], default None\n",
    "        Only collect fragment sizes for the barcodes in barcodes\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with insertsize distributions per barcode.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open fragments file\n",
    "    if utils._is_gz_file(fragments):\n",
    "        f = gzip.open(fragments, \"rt\")\n",
    "    else:\n",
    "        f = open(fragments, \"r\")\n",
    "\n",
    "    # Prepare function for checking against barcodes list\n",
    "    if barcodes is not None:\n",
    "        barcodes = set(barcodes)\n",
    "        check_in = _check_in_list\n",
    "    else:\n",
    "        check_in = _check_true\n",
    "\n",
    "    # Read fragments file and add to dict\n",
    "    print(\"Counting fragment lengths from fragments file...\")\n",
    "    start_time = datetime.datetime.now()\n",
    "    count_dict = {}\n",
    "    for line in f:\n",
    "        columns = line.rstrip().split(\"\\t\")\n",
    "        start = int(columns[1])\n",
    "        end = int(columns[2])\n",
    "        barcode = columns[3]\n",
    "        count = int(columns[4])\n",
    "        size = end - start - 9  # length of insertion (-9 due to to shifted cutting of Tn5)\n",
    "\n",
    "        # Only add fragment if check is true\n",
    "        if check_in(barcode, barcodes) is True:\n",
    "            count_dict = _add_fragment(count_dict, barcode, size, count)\n",
    "\n",
    "    # Fill missing sizes with 0\n",
    "    max_fragment_size = 1001\n",
    "\n",
    "    for barcode in count_dict:\n",
    "        for size in range(max_fragment_size):\n",
    "            if size not in count_dict[barcode]:\n",
    "                count_dict[barcode][size] = 0\n",
    "\n",
    "    # Close file and print elapsed time\n",
    "    end_time = datetime.datetime.now()\n",
    "    elapsed = end_time - start_time\n",
    "    f.close()\n",
    "    print(\"Done reading file - elapsed time: {0}\".format(str(elapsed).split(\".\")[0]))\n",
    "\n",
    "    # Convert dict to pandas dataframe\n",
    "    print(\"Converting counts to dataframe...\")\n",
    "    table = pd.DataFrame.from_dict(count_dict, orient=\"index\")\n",
    "    table = table[[\"insertsize_count\", \"mean_insertsize\"] + sorted(table.columns[2:])]\n",
    "    table[\"mean_insertsize\"] = table[\"mean_insertsize\"].round(2)\n",
    "\n",
    "    print(\"Done getting insertsizes from fragments!\")\n",
    "\n",
    "    return table"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "@beartype\n",
    "def _add_fragment(count_dict: dict[str, int],\n",
    "                  barcode: str,\n",
    "                  size: int,\n",
    "                  count: int = 1) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Add fragment of size 'size' to count_dict.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    count_dict : dict[str, int]\n",
    "        Dictionary containing the counts per insertsize.\n",
    "    barcode : str\n",
    "        Barcode of the read.\n",
    "    size : int\n",
    "        Insertsize to add to count_dict.\n",
    "    count : int, default 1\n",
    "        Number of reads to add to count_dict.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, int]\n",
    "        Updated count_dict\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize if barcode is seen for the first time\n",
    "    if barcode not in count_dict:\n",
    "        count_dict[barcode] = {\"mean_insertsize\": 0, \"insertsize_count\": 0}\n",
    "\n",
    "    # Add read to dict\n",
    "    if size >= 0 and size <= 1000:  # do not save negative insertsize, and set a cap on the maximum insertsize to limit outlier effects\n",
    "\n",
    "        count_dict[barcode][\"insertsize_count\"] += count\n",
    "\n",
    "        # Update mean\n",
    "        mu = count_dict[barcode][\"mean_insertsize\"]\n",
    "        total_count = count_dict[barcode][\"insertsize_count\"]\n",
    "        diff = (size - mu) / total_count\n",
    "        count_dict[barcode][\"mean_insertsize\"] = mu + diff\n",
    "\n",
    "        # Save to distribution\n",
    "        if size not in count_dict[barcode]:  # first time size is seen\n",
    "            count_dict[barcode][size] = 0\n",
    "        count_dict[barcode][size] += count\n",
    "\n",
    "    return count_dict"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HELPERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "@beartype\n",
    "def _is_gz_file(filepath: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check wheather file is a compressed .gz file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the file is a compressed .gz file.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filepath, 'rb') as test_f:\n",
    "        return test_f.read(2) == b'\\x1f\\x8b'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "@beartype\n",
    "def gunzip_file(f_in: str, f_out: str) -> None:\n",
    "    \"\"\"\n",
    "    Decompress file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f_in : str\n",
    "        Path to compressed input file.\n",
    "    f_out : str\n",
    "        Destination to decompressed output file.\n",
    "    \"\"\"\n",
    "\n",
    "    with gzip.open(f_in, 'rb') as h_in:\n",
    "        with open(f_out, 'wb') as h_out:\n",
    "            shutil.copyfileobj(h_in, h_out)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "iterator = pd.read_csv(fragments_file,\n",
    "                       delimiter='\\t',\n",
    "                       header=None,\n",
    "                       names=['chr', 'start', 'stop', 'barcode', 'count'],\n",
    "                       iterator=True,\n",
    "                       chunksize=100000)\n",
    "updated = {}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "chunk = next(iterator)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "chunk"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "check_in = _check_true\n",
    "\n",
    "count_dict = {}\n",
    "\n",
    "for row in chunk.itertuples():\n",
    "    start = int(row[2])\n",
    "    end = int(row[3])\n",
    "    barcode = row[4]\n",
    "    count = int(row[5])\n",
    "    size = end - start - 9  # length of insertion (-9 due to to shifted cutting of Tn5)\n",
    "\n",
    "    # Only add fragment if check is true\n",
    "    if check_in(barcode, barcodes) is True:\n",
    "        count_dict = _add_fragment(count_dict, barcode, size, count)\n",
    "        \n",
    "\n",
    "updated = update_count_dict(updated, count_dict)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def wrap_add_fragments(row, count_dict):\n",
    "    start = int(row[1])\n",
    "    end = int(row[2])\n",
    "    barcode = str(row[3])\n",
    "    count = int(row[4])\n",
    "    size = end - start - 9 \n",
    "\n",
    "    if check_in(barcode, barcodes) is True:\n",
    "        result = _add_fragment(count_dict, barcode, size, count)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "count_dict = {}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "_ = chunk.apply(lambda row: wrap_add_fragments(row, count_dict), axis=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "count_dict"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "check_in = _check_true\n",
    "\n",
    "count_dict = {}\n",
    "\n",
    "for i in range(len(chunk)):\n",
    "    row = chunk.iloc[i]\n",
    "    start = int(row['start'])\n",
    "    end = int(row['stop'])\n",
    "    barcode = row['barcode']\n",
    "    count = int(row['count'])\n",
    "    size = end - start - 9  # length of insertion (-9 due to to shifted cutting of Tn5)\n",
    "\n",
    "    # Only add fragment if check is true\n",
    "    if check_in(barcode, barcodes) is True:\n",
    "        count_dict = _add_fragment(count_dict, barcode, size, count)\n",
    "        \n",
    "\n",
    "updated = update_count_dict(updated, count_dict)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "len(count_dict)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "len(updated)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "pd.DataFrame(updated)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "updated = {}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "pd.DataFrame(count_dict)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "df = pd.DataFrame(count_dict).T"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "df['dist']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "    def _count_fragments_worker(self, chunk, barcodes, check_in):\n",
    "        \n",
    "        count_dict = {}\n",
    "        \n",
    "        for i in range(len(chunk)):\n",
    "            row = chunk.iloc[i]\n",
    "            start = int(row['start'])\n",
    "            end = int(row['stop'])\n",
    "            barcode = row['barcode']\n",
    "            count = int(row['count'])\n",
    "            size = end - start - 9  # length of insertion (-9 due to to shifted cutting of Tn5)\n",
    "\n",
    "            # Only add fragment if check is true\n",
    "            if check_in(barcode, barcodes) is True:\n",
    "                count_dict = self._add_fragment(count_dict, barcode, size, count)\n",
    "                \n",
    "        with self.lock:\n",
    "            self.d = update_count_dict(self.d, count_dict)\n",
    "            \n",
    "    def _check_true(element: Any, alist: Optional[list[Any]] = None) -> bool:  # true regardless of input\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _add_fragment(count_dict: dict[str, int],\n",
    "                      barcode: str,\n",
    "                      size: int,\n",
    "                      count: int = 1,\n",
    "                      max_size=1000):\n",
    "\n",
    "        # Initialize if barcode is seen for the first time\n",
    "        if barcode not in count_dict:\n",
    "            count_dict[barcode] = {\"mean_insertsize\": 0, \"insertsize_count\": 0}\n",
    "\n",
    "        # Add read to dict\n",
    "        if size >= 0 and size <= max_size:  # do not save negative insertsize, and set a cap on the maximum insertsize to limit outlier effects\n",
    "\n",
    "            count_dict[barcode][\"insertsize_count\"] += count\n",
    "\n",
    "            # Update mean\n",
    "            mu = count_dict[barcode][\"mean_insertsize\"]\n",
    "            total_count = count_dict[barcode][\"insertsize_count\"]\n",
    "            diff = (size - mu) / total_count\n",
    "            count_dict[barcode][\"mean_insertsize\"] = mu + diff\n",
    "\n",
    "            # Save to distribution\n",
    "            if size not in count_dict[barcode]:  # first time size is seen\n",
    "                sizes = np.arange(0,max_size+1)\n",
    "                count_dict[barcode]['dist'] = np.zeros(max_size)\n",
    "            count_dict[barcode]['dist'][size] += count\n",
    "            \n",
    "        return count_dict"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "np.arange(0,1001) + np.arange(0,1001)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "count_dict[']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peakqc",
   "language": "python",
   "name": "peakqc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
