{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Counting Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T14:43:39.898178187Z",
     "start_time": "2023-12-15T14:43:39.854159700Z"
    }
   },
   "outputs": [],
   "source": [
    "bamfile = \"/mnt/workspace2/jdetlef/data/public_data/heart_left_ventricle_194_CB_tagged.bam\"\n",
    "fragments_file = \"/mnt/workspace2/jdetlef/data/public_data/fragments_heart_left_ventricle_194_sorted.bed\"\n",
    "h5ad_file = \"/mnt/workspace2/jdetlef/data/public_data/heart_lv_SM-JF1NY.h5ad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T14:38:59.522229322Z",
     "start_time": "2023-12-15T14:38:54.585241821Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to determine R home: [Errno 2] No such file or directory: 'R'\n"
     ]
    }
   ],
   "source": [
    "import sctoolbox.tools as tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:45:37.197738984Z",
     "start_time": "2023-12-15T15:45:37.148652850Z"
    }
   },
   "outputs": [],
   "source": [
    "# individual imports\n",
    "import episcanpy as epi\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import datetime\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Manager, Lock, Pool\n",
    "\n",
    "from beartype import beartype\n",
    "from beartype.typing import Any, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T14:44:38.389758803Z",
     "start_time": "2023-12-15T14:44:34.966900277Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 9110 × 1154611\n",
       "    obs: 'logUMI', 'tsse', 'tissue', 'cell type', 'Life stage', 'closest Cell Ontology term(s)', 'Cell Ontology ID'\n",
       "    var: 'Chromosome', 'hg38_Start', 'hg38_End', 'Class', 'Present in fetal tissues', 'Present in adult tissues', 'CRE module'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata = epi.read_h5ad(h5ad_file)\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T14:46:14.499936670Z",
     "start_time": "2023-12-15T14:46:14.474794834Z"
    }
   },
   "outputs": [],
   "source": [
    "adata_barcodes = adata.obs.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:56:13.154204628Z",
     "start_time": "2023-12-15T15:56:13.136171928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.3 ms, sys: 0 ns, total: 2.3 ms\n",
      "Wall time: 2.31 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# split index for barcodes CBs\n",
    "barcodes = []\n",
    "for entry in adata_barcodes:\n",
    "    barcode = entry.split('+')[1]\n",
    "    barcodes.append(barcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:57:38.120801356Z",
     "start_time": "2023-12-15T15:57:38.072063844Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_lines(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return sum(1 for line in file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:57:54.544122099Z",
     "start_time": "2023-12-15T15:57:39.446642696Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Replace 'yourfile.txt' with the path to your file\n",
    "number_of_lines = count_lines(fragments_file)\n",
    "print(f\"Total number of lines: {number_of_lines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:45:22.195137291Z",
     "start_time": "2023-12-15T15:45:22.182267393Z"
    }
   },
   "outputs": [],
   "source": [
    "small_fragments = '/mnt/workspace2/jdetlef/data/public_data/cropped_heart_fragments.bed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:51:15.418105880Z",
     "start_time": "2023-12-15T15:51:15.388304546Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class MPFragmentCounter():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Init class variables.\"\"\"\n",
    "        \n",
    "        self.m = Manager()\n",
    "        self.d = self.m.dict()\n",
    "        self.lock = Lock()\n",
    "\n",
    "\n",
    "        \n",
    "    def _check_in_list(element: Any, alist: list[Any] | set[Any]) -> bool:\n",
    "        \"\"\"\n",
    "        Check if element is in list.\n",
    "\n",
    "        TODO Do we need this function?\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        element : Any\n",
    "            Element that is checked for.\n",
    "        alist : list[Any] | set[Any]\n",
    "            List or set in which the element is searched for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if element is in list else False\n",
    "        \"\"\"\n",
    "\n",
    "        return element in alist\n",
    "\n",
    "\n",
    "    \n",
    "    def _check_true(element: Any, alist: Optional[list[Any]] = None) -> bool:  # true regardless of input\n",
    "        \"\"\"\n",
    "        Return True regardless of input\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        element : Any\n",
    "            Element that is checked for.\n",
    "        alist: Optional[list[Any]]\n",
    "            List or set in which the element is searched for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if element is in list else False\n",
    "        \"\"\"\n",
    "\n",
    "        return True\n",
    "\n",
    "    \n",
    "    def insertsize_from_fragments(self, fragments: str,\n",
    "                                  barcodes: Optional[list[str]] = None,\n",
    "                                  n_threads: int = 8) -> pd.DataFrame:\n",
    "        # Open fragments file\n",
    "        if _is_gz_file(fragments):\n",
    "            f = gzip.open(fragments, \"rt\")\n",
    "        else:\n",
    "            f = open(fragments, \"r\")\n",
    "\n",
    "        # Prepare function for checking against barcodes list\n",
    "        if barcodes is not None:\n",
    "            barcodes = set(barcodes)\n",
    "            check_in = self._check_in_list\n",
    "        else:\n",
    "            check_in = self._check_true\n",
    "\n",
    "        iterator = pd.read_csv(fragments,\n",
    "                               delimiter='\\t',\n",
    "                               header=None,\n",
    "                               names=['chr', 'start', 'stop', 'barcode', 'count'],\n",
    "                               iterator=True,\n",
    "                               chunksize=1000)\n",
    "\n",
    "        # start timer\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        pool = Pool(n_threads, maxtasksperchild=48)\n",
    "        jobs = []\n",
    "        # split fragments into chunks\n",
    "        for chunk in iterator:\n",
    "            # apply async job wit callback function\n",
    "            job = pool.apply_async(self._count_fragments_worker, args=(chunk, barcodes, check_in))\n",
    "            jobs.append(job)\n",
    "        # monitor progress\n",
    "        # utils.monitor_jobs(jobs, description=\"Progress\")\n",
    "        # close pool\n",
    "        pool.close()\n",
    "        # wait for all jobs to finish\n",
    "        pool.join()\n",
    "        # reset settings\n",
    "        count_dict = self.d\n",
    "        print('what is going on')\n",
    "        print(count_dict)\n",
    "        # Fill missing sizes with 0\n",
    "        max_fragment_size = 1001\n",
    "\n",
    "        for barcode in count_dict:\n",
    "            for size in range(max_fragment_size):\n",
    "                if size not in count_dict[barcode]:\n",
    "                    count_dict[barcode][size] = 0\n",
    "\n",
    "        # Close file and print elapsed time\n",
    "        end_time = datetime.datetime.now()\n",
    "        f.close()\n",
    "\n",
    "        elapsed = end_time - start_time\n",
    "        print(\"Done reading file - elapsed time: {0}\".format(str(elapsed).split(\".\")[0]))\n",
    "\n",
    "        # Convert dict to pandas dataframe\n",
    "        print(\"Converting counts to dataframe...\")\n",
    "        table = pd.DataFrame.from_dict(count_dict, orient=\"index\")\n",
    "        table = table[[\"insertsize_count\", \"mean_insertsize\"] + sorted(table.columns[2:])]\n",
    "        table[\"mean_insertsize\"] = table[\"mean_insertsize\"].round(2)\n",
    "\n",
    "        print(\"Done getting insertsizes from fragments!\")\n",
    "\n",
    "        return table\n",
    "\n",
    "    \n",
    "    def _count_fragments_worker(self, chunk, barcodes, check_in):\n",
    "        \n",
    "        count_dict = {}\n",
    "        \n",
    "        for i in range(len(chunk)):\n",
    "            row = chunk.iloc[i]\n",
    "            start = int(row['start'])\n",
    "            end = int(row['stop'])\n",
    "            barcode = row['barcode']\n",
    "            count = int(row['count'])\n",
    "            size = end - start - 9  # length of insertion (-9 due to to shifted cutting of Tn5)\n",
    "\n",
    "            # Only add fragment if check is true\n",
    "            if check_in(barcode, barcodes) is True:\n",
    "                count_dict = self._add_fragment(count_dict, barcode, size, count)\n",
    "                \n",
    "        with self.lock:\n",
    "            self.d = update_count_dict(self.d, count_dict)\n",
    "\n",
    "\n",
    "    def _add_fragment(count_dict: dict[str, int],\n",
    "                      barcode: str,\n",
    "                      size: int,\n",
    "                      count: int = 1):\n",
    "        \"\"\"\n",
    "        Add fragment of size 'size' to count_dict.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        count_dict : dict[str, int]\n",
    "            Dictionary containing the counts per insertsize.\n",
    "        barcode : str\n",
    "            Barcode of the read.\n",
    "        size : int\n",
    "            Insertsize to add to count_dict.\n",
    "        count : int, default 1\n",
    "            Number of reads to add to count_dict.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize if barcode is seen for the first time\n",
    "        if barcode not in count_dict:\n",
    "            count_dict[barcode] = {\"mean_insertsize\": 0, \"insertsize_count\": 0}\n",
    "\n",
    "        # Add read to dict\n",
    "        if size >= 0 and size <= 1000:  # do not save negative insertsize, and set a cap on the maximum insertsize to limit outlier effects\n",
    "\n",
    "            count_dict[barcode][\"insertsize_count\"] += count\n",
    "\n",
    "            # Update mean\n",
    "            mu = count_dict[barcode][\"mean_insertsize\"]\n",
    "            total_count = count_dict[barcode][\"insertsize_count\"]\n",
    "            diff = (size - mu) / total_count\n",
    "            count_dict[barcode][\"mean_insertsize\"] = mu + diff\n",
    "\n",
    "            # Save to distribution\n",
    "            if size not in count_dict[barcode]:  # first time size is seen\n",
    "                count_dict[barcode][size] = 0\n",
    "            count_dict[barcode][size] += count\n",
    "            \n",
    "        return count_dict\n",
    "    \n",
    "\n",
    "    def _log_result(self, result: Any) -> None:\n",
    "        \"\"\"Log results from mp_counter.\"\"\"\n",
    "\n",
    "        if self.merged_dict:\n",
    "            self.merged_dict = dict(Counter(self.merged_dict) + Counter(result))\n",
    "            # print('merging')\n",
    "        else:\n",
    "            self.merged_dict = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    " mpc = MPFragmentCounter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is going on\n",
      "{}\n",
      "Done reading file - elapsed time: 0:00:00\n",
      "Converting counts to dataframe...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['insertsize_count', 'mean_insertsize'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "Cell \u001b[0;32mIn[127], line 118\u001b[0m, in \u001b[0;36mMPFragmentCounter.insertsize_from_fragments\u001b[0;34m(self, fragments, barcodes, n_threads)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting counts to dataframe...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    117\u001b[0m table \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(count_dict, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 118\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minsertsize_count\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean_insertsize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    119\u001b[0m table[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_insertsize\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m table[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_insertsize\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone getting insertsizes from fragments!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/dev-sctoolbox/lib/python3.10/site-packages/pandas/core/frame.py:3811\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3810\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3811\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/dev-sctoolbox/lib/python3.10/site-packages/pandas/core/indexes/base.py:6113\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6111\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6113\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6115\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6117\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dev-sctoolbox/lib/python3.10/site-packages/pandas/core/indexes/base.py:6173\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6172\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 6173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6175\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6176\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['insertsize_count', 'mean_insertsize'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "%%time\n",
    "counts = mpc.insertsize_from_fragments(small_fragments, barcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dict_1={}\n",
    "count_dict_1['AB'] = {\"mean_insertsize\": 10, \"insertsize_count\": 5}\n",
    "count_dict_1['BC'] = {\"mean_insertsize\": 10, \"insertsize_count\": 20}\n",
    "\n",
    "count_dict_2={}\n",
    "count_dict_2['AB'] = {\"mean_insertsize\": 20, \"insertsize_count\": 20}\n",
    "count_dict_2['BC'] = {\"mean_insertsize\": 20, \"insertsize_count\": 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_count_dict(count_dict_1, count_dict_2):\n",
    "    \"\"\"\n",
    "    updates\n",
    "    \"\"\"\n",
    "    \n",
    "    # make Dataframes for computation\n",
    "    df1 = pd.DataFrame(count_dict_1).T\n",
    "    df2 = pd.DataFrame(count_dict_2).T\n",
    "\n",
    "    # merge counts\n",
    "    merged_counts = pd.merge(df1[\"insertsize_count\"], df2[\"insertsize_count\"], left_index=True, right_index=True)\n",
    "    # sum total counts/barcode\n",
    "    updated_counts = merged_counts.sum(axis=1)\n",
    "\n",
    "    # calculate scaling factors\n",
    "    x_scaling_factor = merged_counts[\"insertsize_count_x\"] / updated_counts\n",
    "    y_scaling_factor = merged_counts[\"insertsize_count_y\"] / updated_counts\n",
    "\n",
    "    # merge mean insertsizes\n",
    "    merged_mean_insertsizes = pd.merge(df1[\"mean_insertsize\"], df2[\"mean_insertsize\"], left_index=True, right_index=True)\n",
    "\n",
    "    # scale mean insertsizes\n",
    "    merged_mean_insertsizes[\"mean_insertsize_x\"] = merged_mean_insertsizes[\"mean_insertsize_x\"] * x_scaling_factor\n",
    "    merged_mean_insertsizes[\"mean_insertsize_y\"] = merged_mean_insertsizes[\"mean_insertsize_y\"] * y_scaling_factor\n",
    "\n",
    "    # sum the scaled means\n",
    "    updated_means = merged_mean_insertsizes.sum(axis=1)\n",
    "\n",
    "    # build the updated dictionary\n",
    "    updated_dict = pd.DataFrame({'mean_insertsizes': updated_means, 'insertsize_counts' : updated_counts}).T.to_dict()\n",
    "    \n",
    "    \n",
    "    return updated_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_insertsizes</th>\n",
       "      <th>insertsize_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AB</th>\n",
       "      <td>18.0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BC</th>\n",
       "      <td>12.0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_insertsizes  insertsize_counts\n",
       "AB              18.0                 25\n",
       "BC              12.0                 25"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'mean_insertsizes': updated_means, 'insertsize_counts' : updated_counts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>insertsize_count_x</th>\n",
       "      <th>insertsize_count_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AB</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BC</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    insertsize_count_x  insertsize_count_y\n",
       "AB                   4                   2\n",
       "BC                   5                   3"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_insertsizes = pd.merge(df1[\"insertsize_count\"], df2[\"insertsize_count\"], left_index=True, right_index=True)\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaling_factor = merged_insertsizes[\"insertsize_count_x\"] / merged_insertsizes.sum(axis=1)\n",
    "y_scaling_factor = merged_insertsizes[\"insertsize_count_y\"] / merged_insertsizes.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_insertsize_x</th>\n",
       "      <th>mean_insertsize_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AB</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BC</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_insertsize_x  mean_insertsize_y\n",
       "AB                 10                 20\n",
       "BC                 10                 20"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_mean_insertsizes = pd.merge(df1[\"mean_insertsize\"], df2[\"mean_insertsize\"], left_index=True, right_index=True)\n",
    "merged_mean_insertsizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_mean_insertsizes[\"mean_insertsize_x\"] = merged_mean_insertsizes[\"mean_insertsize_x\"] * x_scaling_factor\n",
    "merged_mean_insertsizes[\"mean_insertsize_y\"] = merged_mean_insertsizes[\"mean_insertsize_y\"] * y_scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AB    18.0\n",
       "BC    12.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_mean_insertsizes.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_mean_insertsizes * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a    5\n",
      "b    7\n",
      "c    9\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Erstellen Sie zwei Beispieldatenframes\n",
    "df1 = pd.DataFrame({'Werte1': [1, 2, 3]}, index=['a', 'b', 'c'])\n",
    "df2 = pd.DataFrame({'Werte1': [4, 5, 6]}, index=['a', 'b', 'c'])\n",
    "\n",
    "# Mergen Sie die DataFrames am Index\n",
    "merged_df = pd.merge(df1, df2, left_index=True, right_index=True)\n",
    "\n",
    "# Summieren Sie die Werte\n",
    "summed_df = merged_df.sum(axis=1)\n",
    "\n",
    "print(summed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Werte1_x</th>\n",
       "      <th>Werte1_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Werte1_x  Werte1_y\n",
       "a         1         4\n",
       "b         2         5\n",
       "c         3         6"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T14:54:55.792910295Z",
     "start_time": "2023-12-15T14:49:43.045852210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Counting fragment lengths from fragments file...\n",
      "[INFO] Done reading file - elapsed time: 0:00:00\n",
      "[INFO] Converting counts to dataframe...\n",
      "[INFO] Done getting insertsizes from fragments!\n",
      "CPU times: user 2.64 s, sys: 377 ms, total: 3.02 s\n",
      "Wall time: 2.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count_table = tools._insertsize_from_fragments(small_fragments, barcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@beartype\n",
    "def _insertsize_from_fragments(fragments: str,\n",
    "                               barcodes: Optional[list[str]] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get fragment insertsize distributions per barcode from fragments file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fragments : str\n",
    "        Path to fragments.bed(.gz) file.\n",
    "    barcodes : Optional[list[str]], default None\n",
    "        Only collect fragment sizes for the barcodes in barcodes\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with insertsize distributions per barcode.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open fragments file\n",
    "    if utils._is_gz_file(fragments):\n",
    "        f = gzip.open(fragments, \"rt\")\n",
    "    else:\n",
    "        f = open(fragments, \"r\")\n",
    "\n",
    "    # Prepare function for checking against barcodes list\n",
    "    if barcodes is not None:\n",
    "        barcodes = set(barcodes)\n",
    "        check_in = _check_in_list\n",
    "    else:\n",
    "        check_in = _check_true\n",
    "\n",
    "    # Read fragments file and add to dict\n",
    "    print(\"Counting fragment lengths from fragments file...\")\n",
    "    start_time = datetime.datetime.now()\n",
    "    count_dict = {}\n",
    "    for line in f:\n",
    "        columns = line.rstrip().split(\"\\t\")\n",
    "        start = int(columns[1])\n",
    "        end = int(columns[2])\n",
    "        barcode = columns[3]\n",
    "        count = int(columns[4])\n",
    "        size = end - start - 9  # length of insertion (-9 due to to shifted cutting of Tn5)\n",
    "\n",
    "        # Only add fragment if check is true\n",
    "        if check_in(barcode, barcodes) is True:\n",
    "            count_dict = _add_fragment(count_dict, barcode, size, count)\n",
    "\n",
    "    # Fill missing sizes with 0\n",
    "    max_fragment_size = 1001\n",
    "\n",
    "    for barcode in count_dict:\n",
    "        for size in range(max_fragment_size):\n",
    "            if size not in count_dict[barcode]:\n",
    "                count_dict[barcode][size] = 0\n",
    "\n",
    "    # Close file and print elapsed time\n",
    "    end_time = datetime.datetime.now()\n",
    "    elapsed = end_time - start_time\n",
    "    f.close()\n",
    "    print(\"Done reading file - elapsed time: {0}\".format(str(elapsed).split(\".\")[0]))\n",
    "\n",
    "    # Convert dict to pandas dataframe\n",
    "    print(\"Converting counts to dataframe...\")\n",
    "    table = pd.DataFrame.from_dict(count_dict, orient=\"index\")\n",
    "    table = table[[\"insertsize_count\", \"mean_insertsize\"] + sorted(table.columns[2:])]\n",
    "    table[\"mean_insertsize\"] = table[\"mean_insertsize\"].round(2)\n",
    "\n",
    "    print(\"Done getting insertsizes from fragments!\")\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@beartype\n",
    "def _add_fragment(count_dict: dict[str, int],\n",
    "                  barcode: str,\n",
    "                  size: int,\n",
    "                  count: int = 1) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Add fragment of size 'size' to count_dict.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    count_dict : dict[str, int]\n",
    "        Dictionary containing the counts per insertsize.\n",
    "    barcode : str\n",
    "        Barcode of the read.\n",
    "    size : int\n",
    "        Insertsize to add to count_dict.\n",
    "    count : int, default 1\n",
    "        Number of reads to add to count_dict.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, int]\n",
    "        Updated count_dict\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize if barcode is seen for the first time\n",
    "    if barcode not in count_dict:\n",
    "        count_dict[barcode] = {\"mean_insertsize\": 0, \"insertsize_count\": 0}\n",
    "\n",
    "    # Add read to dict\n",
    "    if size >= 0 and size <= 1000:  # do not save negative insertsize, and set a cap on the maximum insertsize to limit outlier effects\n",
    "\n",
    "        count_dict[barcode][\"insertsize_count\"] += count\n",
    "\n",
    "        # Update mean\n",
    "        mu = count_dict[barcode][\"mean_insertsize\"]\n",
    "        total_count = count_dict[barcode][\"insertsize_count\"]\n",
    "        diff = (size - mu) / total_count\n",
    "        count_dict[barcode][\"mean_insertsize\"] = mu + diff\n",
    "\n",
    "        # Save to distribution\n",
    "        if size not in count_dict[barcode]:  # first time size is seen\n",
    "            count_dict[barcode][size] = 0\n",
    "        count_dict[barcode][size] += count\n",
    "\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HELPERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@beartype\n",
    "def _is_gz_file(filepath: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check wheather file is a compressed .gz file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the file is a compressed .gz file.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filepath, 'rb') as test_f:\n",
    "        return test_f.read(2) == b'\\x1f\\x8b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@beartype\n",
    "def gunzip_file(f_in: str, f_out: str) -> None:\n",
    "    \"\"\"\n",
    "    Decompress file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f_in : str\n",
    "        Path to compressed input file.\n",
    "    f_out : str\n",
    "        Destination to decompressed output file.\n",
    "    \"\"\"\n",
    "\n",
    "    with gzip.open(f_in, 'rb') as h_in:\n",
    "        with open(f_out, 'wb') as h_out:\n",
    "            shutil.copyfileobj(h_in, h_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-sctoolbox",
   "language": "python",
   "name": "dev-sctoolbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
